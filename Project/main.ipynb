{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "os.environ['QT_QPA_PLATFORM'] = 'offscreen'\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Ensure Dataset Path Exists\n",
    "dataset_path = \"dataset/data.yaml\"  # Update with your dataset path\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset YAML file not found: {dataset_path}\")\n",
    "\n",
    "# Load Pretrained YOLOv8 Model\n",
    "model = YOLO('yolov8n.pt')  # Use 'yolov8n.pt', 'yolov8s.pt', etc., based on your hardware capacity\n",
    "\n",
    "# Train the Model\n",
    "results = model.train(\n",
    "    data=dataset_path,       # Path to dataset YAML\n",
    "    epochs=1,                # Number of epochs\n",
    "    batch=16,                # Batch size\n",
    "    imgsz=640,               # Image size\n",
    "    workers=4,               # Number of data loading workers\n",
    "    optimizer='AdamW',       # Optimizer\n",
    "    lr0=0.001,               # Initial learning rate\n",
    "    patience=10,             # Early stopping\n",
    "    augment=True,            # Data augmentation\n",
    "    val=True                 # Validate after each epoch\n",
    ")\n",
    "\n",
    "# Plot Training Loss\n",
    "if hasattr(results, 'metrics'):\n",
    "    metrics = results.metrics\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(metrics.get('box_loss', []), label=\"Box Loss\")\n",
    "    plt.plot(metrics.get('obj_loss', []), label=\"Object Loss\")\n",
    "    plt.plot(metrics.get('cls_loss', []), label=\"Classification Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Training metrics are not available for plotting.\")\n",
    "\n",
    "# Save the Best Model\n",
    "best_model_path = 'chair_detection/yolov8_chair/weights/best.pt'\n",
    "os.makedirs(os.path.dirname(best_model_path), exist_ok=True)\n",
    "model.save(best_model_path)\n",
    "print(f\"Training complete. Best model saved at {best_model_path}\")\n",
    "\n",
    "# Load Trained Model\n",
    "model = YOLO(best_model_path)\n",
    "\n",
    "# Function to Detect and Count Objects\n",
    "def detect_and_count(image, model, class_name=\"chair\"):\n",
    "    results = model(image)  # Run detection\n",
    "    detections = results[0].boxes  # Extract bounding boxes\n",
    "    chair_count = 0\n",
    "\n",
    "    # Process each detection\n",
    "    for detection in detections:\n",
    "        class_id = int(detection.cls)  # Class ID\n",
    "        if model.names[class_id] == class_name:  # Match the target class\n",
    "            chair_count += 1\n",
    "            # Draw bounding box and label\n",
    "            x1, y1, x2, y2 = map(int, detection.xyxy[0])  # Bounding box coordinates\n",
    "            conf = detection.conf  # Confidence score\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                image,\n",
    "                f\"{class_name} {conf:.2f}\",\n",
    "                (x1, y1 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "            )\n",
    "    return image, chair_count\n",
    "\n",
    "# Process Images and Save Results\n",
    "def process_images(input_dir, output_dir, model, class_name=\"chair\"):\n",
    "    results_summary = []\n",
    "    supported_formats = (\".jpg\", \".png\", \".jpeg\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for img_file in tqdm(os.listdir(input_dir), desc=\"Processing images\"):\n",
    "        if not img_file.endswith(supported_formats):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(input_dir, img_file)\n",
    "        try:\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                raise ValueError(\"Invalid image format or file.\")\n",
    "\n",
    "            annotated_image, count = detect_and_count(image, model, class_name)\n",
    "            output_path = os.path.join(output_dir, img_file)\n",
    "            cv2.imwrite(output_path, annotated_image)\n",
    "            results_summary.append({\"image\": img_file, \"chair_count\": count})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {e}\")\n",
    "\n",
    "    summary_path = os.path.join(output_dir, \"results_summary.json\")\n",
    "    with open(summary_path, \"w\") as json_file:\n",
    "        json.dump(results_summary, json_file, indent=4)\n",
    "\n",
    "    print(f\"Processing complete. Results saved to {summary_path}\")\n",
    "\n",
    "\n",
    "# Save Detections as Video\n",
    "def save_detection_video(input_dir, output_video_path, model, class_name=\"chair\"):\n",
    "    supported_formats = (\".jpg\", \".png\", \".jpeg\")\n",
    "    frame_rate = 10  # Adjust frame rate as needed\n",
    "\n",
    "    # Validate input images\n",
    "    valid_images = [\n",
    "        os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(supported_formats)\n",
    "    ]\n",
    "    if not valid_images:\n",
    "        raise ValueError(\"No valid image files found in the directory.\")\n",
    "\n",
    "    # Determine frame size\n",
    "    first_frame = cv2.imread(valid_images[0])\n",
    "    height, width, _ = first_frame.shape\n",
    "\n",
    "    # Define video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (width, height))\n",
    "\n",
    "    for img_file in tqdm(valid_images, desc=\"Creating Video\"):\n",
    "        try:\n",
    "            image = cv2.imread(img_file)\n",
    "            annotated_image, count = detect_and_count(image, model, class_name)\n",
    "            video_writer.write(annotated_image)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {e}\")\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved at {output_video_path}\")\n",
    "\n",
    "# Input and Output Directories\n",
    "INPUT_IMAGES_DIR = \"dataset/images/test\"  # Replace with your image directory\n",
    "OUTPUT_IMAGES_DIR = \"detected\"  # Replace with output directory\n",
    "os.makedirs(OUTPUT_IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "# Process and Save Annotated Images\n",
    "process_images(INPUT_IMAGES_DIR, OUTPUT_IMAGES_DIR, model)\n",
    "\n",
    "# Save Detection Results as Video\n",
    "save_detection_video(INPUT_IMAGES_DIR, \"detections_output.mp4\", model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
