{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set Paths for Dataset\n",
    "dataset_path = \"dataset/data.yaml\"  # Update with your dataset's path\n",
    "\n",
    "# Load Pretrained YOLOv8 Model\n",
    "model = YOLO('yolov8n.pt')  # Choose 'yolov8n.pt', 'yolov8s.pt', 'yolov8m.pt', etc., based on your compute capacity\n",
    "\n",
    "# 1. Real-Time Loss Visualization\n",
    "class TrainingVisualizer:\n",
    "    def __init__(self):\n",
    "        self.epoch_loss = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        loss = logs['train_loss']  # Logs contain training loss for the epoch\n",
    "        self.epoch_loss.append(loss)\n",
    "\n",
    "        # Update live plot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(self.epoch_loss) + 1), self.epoch_loss, label=\"Train Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss Over Epochs\")\n",
    "        plt.legend()\n",
    "        plt.pause(0.1)  # Pause to update dynamically\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Instantiate the visualizer\n",
    "visualizer = TrainingVisualizer()\n",
    "\n",
    "# Train the Model\n",
    "results = model.train(\n",
    "    data=dataset_path,       # Path to the dataset YAML file\n",
    "    epochs=5,                # Number of epochs (increase for better results)\n",
    "    batch=16,                # Batch size (adjust based on GPU memory)\n",
    "    imgsz=1280,              # Image size (higher can increase accuracy but requires more memory)\n",
    "    workers=4,               # Number of data loading workers\n",
    "    optimizer='AdamW',       # Use AdamW optimizer for better convergence\n",
    "    lr0=0.001,               # Initial learning rate\n",
    "    patience=10,             # Early stopping patience\n",
    "    augment=True,            # Apply data augmentation (random scaling, flipping, etc.)\n",
    "    val=True,                # Validate after training\n",
    "    callbacks=[visualizer]   # Add visualizer callback\n",
    ")\n",
    "\n",
    "# Save the best model weights\n",
    "best_model_path = 'chair_detection/yolov8_chair/weights/best.pt'\n",
    "print(f\"Training complete. Best model saved at {best_model_path}\")\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "MODEL_PATH = \"chair_detection/yolov8_chair/weights/best.pt\"\n",
    "model = YOLO(MODEL_PATH)\n",
    "\n",
    "\n",
    "# Function to detect and count chairs (and annotate images)\n",
    "def detect_and_count_chairs(image, model, class_name=\"chair\"):\n",
    "    results = model(image)  # Run detection\n",
    "    detections = results[0].boxes  # Extract bounding boxes\n",
    "    chair_count = 0\n",
    "\n",
    "    # Iterate over detections\n",
    "    for detection in detections:\n",
    "        class_id = int(detection.cls[0])  # Class ID\n",
    "        if model.names[class_id] == class_name:  # Match the target class\n",
    "            chair_count += 1\n",
    "            # Draw bounding box and label\n",
    "            x1, y1, x2, y2 = map(int, detection.xyxy[0])  # Bounding box\n",
    "            conf = detection.conf[0]  # Confidence score\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                image,\n",
    "                f\"{class_name} {conf:.2f}\",\n",
    "                (x1, y1 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "            )\n",
    "    return image, chair_count\n",
    "\n",
    "\n",
    "# Main function to process and save images with results summary\n",
    "def process_images(input_dir, output_dir, model, class_name=\"chair\"):\n",
    "    results_summary = []\n",
    "    supported_formats = (\".jpg\", \".png\", \".jpeg\")\n",
    "\n",
    "    # Process each image\n",
    "    for img_file in tqdm(os.listdir(input_dir), desc=\"Processing images\"):\n",
    "        if not img_file.endswith(supported_formats):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(input_dir, img_file)\n",
    "        try:\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                raise ValueError(\"Invalid image format or file.\")\n",
    "\n",
    "            # Detect and count chairs\n",
    "            annotated_image, count = detect_and_count_chairs(image, model, class_name)\n",
    "\n",
    "            # Save the annotated image\n",
    "            output_path = os.path.join(output_dir, img_file)\n",
    "            cv2.imwrite(output_path, annotated_image)\n",
    "\n",
    "            # Append results for this image\n",
    "            results_summary.append({\"image\": img_file, \"chair_count\": count})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {e}\")\n",
    "\n",
    "    # Save results summary to a JSON file\n",
    "    summary_path = os.path.join(output_dir, \"results_summary.json\")\n",
    "    with open(summary_path, \"w\") as json_file:\n",
    "        json.dump(results_summary, json_file, indent=4)\n",
    "\n",
    "    print(f\"Processing complete. Results saved to {summary_path}\")\n",
    "\n",
    "\n",
    "# Live Detection Display\n",
    "def live_detection_display(input_dir, model, class_name=\"chair\"):\n",
    "    supported_formats = (\".jpg\", \".png\", \".jpeg\")\n",
    "    window_name = \"Live Detection Results\"\n",
    "\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(window_name, 800, 600)\n",
    "\n",
    "    for img_file in tqdm(os.listdir(input_dir), desc=\"Live Detection\"):\n",
    "        if not img_file.endswith(supported_formats):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(input_dir, img_file)\n",
    "        try:\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                raise ValueError(\"Invalid image format or file.\")\n",
    "\n",
    "            # Detect and count chairs\n",
    "            annotated_image, count = detect_and_count_chairs(image, model, class_name)\n",
    "\n",
    "            # Display annotated image\n",
    "            cv2.imshow(window_name, annotated_image)\n",
    "            if cv2.waitKey(500) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {e}\")\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Save Detection as Video\n",
    "def save_detection_video(input_dir, output_video_path, model, class_name=\"chair\"):\n",
    "    supported_formats = (\".jpg\", \".png\", \".jpeg\")\n",
    "    frame_rate = 10  # Adjust as needed\n",
    "\n",
    "    # Get first image to determine frame size\n",
    "    first_img = next(\n",
    "        (os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(supported_formats)),\n",
    "        None,\n",
    "    )\n",
    "    if not first_img:\n",
    "        raise ValueError(\"No valid image files in directory.\")\n",
    "    \n",
    "    first_frame = cv2.imread(first_img)\n",
    "    height, width, _ = first_frame.shape\n",
    "\n",
    "    # Define video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (width, height))\n",
    "\n",
    "    for img_file in tqdm(os.listdir(input_dir), desc=\"Creating Video\"):\n",
    "        if not img_file.endswith(supported_formats):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(input_dir, img_file)\n",
    "        try:\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                raise ValueError(\"Invalid image format or file.\")\n",
    "\n",
    "            # Detect and annotate image\n",
    "            annotated_image, count = detect_and_count_chairs(image, model, class_name)\n",
    "\n",
    "            # Write frame to video\n",
    "            video_writer.write(annotated_image)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {e}\")\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved at {output_video_path}\")\n",
    "\n",
    "\n",
    "# Input and output directories\n",
    "INPUT_IMAGES_DIR = \"path/to/your/images\"  # Replace with your image directory\n",
    "OUTPUT_IMAGES_DIR = \"path/to/save/detections\"  # Replace with output directory\n",
    "os.makedirs(OUTPUT_IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "# Process and save annotated images\n",
    "process_images(INPUT_IMAGES_DIR, OUTPUT_IMAGES_DIR, model)\n",
    "\n",
    "# Live display of detections\n",
    "live_detection_display(INPUT_IMAGES_DIR, model)\n",
    "\n",
    "# Save detections as video\n",
    "save_detection_video(INPUT_IMAGES_DIR, \"detections_output.mp4\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
